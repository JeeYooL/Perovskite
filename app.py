import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import io
import re

# ë¨¸ì‹ ëŸ¬ë‹ & ì„¤ëª… ê°€ëŠ¥í•œ AI(XAI) ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import r2_score, mean_absolute_error
import xgboost as xgb
import shap

# -------------------------------------------------------------------
# í˜ì´ì§€ ì„¤ì •
# -------------------------------------------------------------------
st.set_page_config(
    page_title="Perovskite AI Lab (XGBoost + SHAP)",
    page_icon="ğŸ§ª",
    layout="wide"
)

# ìŠ¤íƒ€ì¼ ì»¤ìŠ¤í…€ (ë…¼ë¬¸ ìŠ¤íƒ€ì¼ì˜ ê¹”ë”í•œ ë””ìì¸)
st.markdown("""
    <style>
    .main { background-color: #ffffff; }
    h1, h2, h3 { color: #003366; font-family: 'Arial', sans-serif; }
    .stMetric { background-color: #f0f2f6; padding: 10px; border-radius: 5px; }
    </style>
""", unsafe_allow_html=True)

# -------------------------------------------------------------------
# í•¨ìˆ˜ ì •ì˜
# -------------------------------------------------------------------

def load_data(uploaded_files):
    """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³‘í•©"""
    all_dfs = []
    for uploaded_file in uploaded_files:
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
                all_dfs.append(df)
            elif uploaded_file.name.endswith('.xlsx'):
                df = pd.read_excel(uploaded_file)
                all_dfs.append(df)
        except Exception as e:
            st.error(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({uploaded_file.name}): {e}")
    
    if all_dfs:
        return pd.concat(all_dfs, ignore_index=True)
    return None

def preprocess_data(df):
    """ë°ì´í„° ì „ì²˜ë¦¬: ê²°ì¸¡ì¹˜ ì œê±°, íƒ€ê²Ÿ ë¶„ë¦¬, MLB(Multi-Label Binarization)"""
    target_column = 'PCE (%)'
    
    if target_column not in df.columns:
        st.error(f"ë°ì´í„°ì— '{target_column}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None, None, None, None

    df_cleaned = df.dropna(subset=[target_column]).copy()
    
    # Data Leakage ë°©ì§€: ê²°ê³¼ê°’ ì»¬ëŸ¼ ì œì™¸
    drop_cols = [
        'PCE (%)', 'Voc (V)', 'Jsc (mA/cm2)', 'FF (%)', 'Rs (Î©Â·cmÂ²)', 'Rsh (Î©Â·cmÂ²)',
        'Sample', 'File', 'Scan Direction', 'Unnamed: 0'
    ]
    cols_to_drop = [c for c in drop_cols if c in df_cleaned.columns]
    
    X_raw = df_cleaned.drop(columns=cols_to_drop, errors='ignore')
    y = df_cleaned[target_column]
    
    # ìˆ˜ì¹˜í˜•/ë²”ì£¼í˜• ë¶„ë¦¬
    X_numeric = X_raw.select_dtypes(exclude=['object'])
    X_categorical = X_raw.select_dtypes(include=['object'])
    
    all_processed_dfs = [X_numeric]

    for col in X_categorical.columns:
        # 'FAI + MACl' ê°™ì€ ë³µí•© ì¡°ì„±ì„ ê°œë³„ ì„±ë¶„ìœ¼ë¡œ ë¶„ë¦¬ (One-Hot Encoding í™•ì¥)
        binarized = X_categorical[col].fillna('').astype(str).str.get_dummies(sep=' + ')
        binarized = binarized.add_prefix(f"{col}_")
        all_processed_dfs.append(binarized)
        
    X_processed = pd.concat(all_processed_dfs, axis=1).fillna(0)
    
    # [ìˆ˜ì •ë¨] XGBoost í˜¸í™˜ì„±ì„ ìœ„í•´ **ëª¨ë“  ì»¬ëŸ¼ëª…**ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±° (ìˆ˜ì¹˜í˜• ë³€ìˆ˜ í¬í•¨)
    X_processed.columns = X_processed.columns.str.replace(r'[^\w\s]', '_', regex=True).str.replace(r'\s+', '_', regex=True)
    
    return X_processed, y, df_cleaned, X_raw

# -------------------------------------------------------------------
# ë©”ì¸ UI
# -------------------------------------------------------------------

st.title("ğŸ§ª Perovskite AI Lab: XGBoost & SHAP Analysis")
st.markdown("""
ìµœì‹  ì—°êµ¬ íŠ¸ë Œë“œ(Science, Nature Energy ë“±)ë¥¼ ë°˜ì˜í•˜ì—¬ **XGBoost(ê³ ì„±ëŠ¥ ë¶€ìŠ¤íŒ…)** ëª¨ë¸ê³¼ **SHAP(ì„¤ëª… ê°€ëŠ¥í•œ AI)** ê¸°ë²•ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.
""")
st.markdown("---")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("1. Data Upload")
    uploaded_files = st.file_uploader("Upload CSV/Excel", type=['csv', 'xlsx'], accept_multiple_files=True)
    
    st.header("2. Model Settings")
    test_size = st.slider("Test Set Ratio", 0.1, 0.4, 0.2, 0.05)
    cv_folds = st.slider("CV Folds", 2, 10, 5)
    
    st.markdown("---")
    st.info("ğŸ’¡ **XGBoost**ëŠ” í˜ë¡œë¸ŒìŠ¤ì¹´ì´íŠ¸ ê³µì • ë°ì´í„°ì™€ ê°™ì€ ì •í˜• ë°ì´í„°(Tabular Data)ì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")

if uploaded_files:
    raw_df = load_data(uploaded_files)
    
    if raw_df is not None:
        st.write(f"âœ… Loaded **{len(raw_df)}** samples.")
        
        if st.button("ğŸš€ Run AI Analysis (Train & Explain)"):
            with st.spinner('Preprocessing data & optimizing XGBoost model...'):
                
                # 1. ì „ì²˜ë¦¬
                X, y, df_clean, X_raw_origin = preprocess_data(raw_df)
                
                if X is not None:
                    # 2. ë°ì´í„° ë¶„í• 
                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
                    
                    # 3. XGBoost ëª¨ë¸ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
                    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_jobs=-1, random_state=42)
                    
                    # ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
                    # ë°ì´í„°ê°€ ì ì„ ê²½ìš° ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ max_depthë¥¼ ë‚®ì¶”ê³  n_estimatorsë¥¼ ì¤„ì„
                    param_grid = {
                        'n_estimators': [100, 200, 300],
                        'learning_rate': [0.01, 0.05, 0.1],
                        'max_depth': [3, 5],
                        'subsample': [0.8, 1.0]
                    }
                    
                    grid_search = GridSearchCV(
                        xgb_model, 
                        param_grid, 
                        cv=cv_folds, 
                        scoring='neg_mean_absolute_error',
                        verbose=1,
                        error_score='raise' # ì—ëŸ¬ ë°œìƒ ì‹œ ë¬´ì‹œí•˜ì§€ ì•Šê³  ì¶œë ¥
                    )
                    
                    try:
                        grid_search.fit(X_train, y_train)
                        best_model = grid_search.best_estimator_
                        
                        # ----------------------------------------------------------------
                        # ê²°ê³¼ ëŒ€ì‹œë³´ë“œ
                        # ----------------------------------------------------------------
                        
                        # [Tab 1: ì„±ëŠ¥]
                        st.subheader("1. Model Performance")
                        col1, col2, col3 = st.columns(3)
                        
                        y_pred = best_model.predict(X_test)
                        r2 = r2_score(y_test, y_pred)
                        mae = mean_absolute_error(y_test, y_pred)
                        cv_r2 = cross_val_score(best_model, X, y, cv=cv_folds, scoring='r2').mean()
                        
                        col1.metric("Test RÂ² Score", f"{r2:.4f}")
                        col2.metric("Mean Absolute Error", f"{mae:.4f} %")
                        col3.metric("Cross-Validation RÂ²", f"{cv_r2:.4f}")
                        
                        # ì˜ˆì¸¡ ê·¸ë˜í”„
                        fig, ax = plt.subplots(figsize=(8, 4))
                        ax.scatter(y_test, y_pred, alpha=0.6, color='#2c3e50', edgecolors='w')
                        ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
                        ax.set_xlabel("Experimental PCE (%)")
                        ax.set_ylabel("Predicted PCE (%)")
                        ax.set_title("Prediction Accuracy")
                        ax.grid(True, alpha=0.3)
                        st.pyplot(fig)
                        
                        # [Tab 2: SHAP ë¶„ì„ (XAI)]
                        st.markdown("---")
                        st.subheader("2. Explainable AI (SHAP Analysis)")
                        st.markdown("""
                        **SHAP Summary Plot**ì€ ê° ê³µì • ë³€ìˆ˜ê°€ íš¨ìœ¨ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
                        * **ì ì˜ ìƒ‰ìƒ**: ë³€ìˆ˜ì˜ ê°’ (ë¹¨ê°•=ë†’ìŒ, íŒŒë‘=ë‚®ìŒ)
                        * **Xì¶• ìœ„ì¹˜**: íš¨ìœ¨ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ (ì˜¤ë¥¸ìª½=íš¨ìœ¨ ì¦ê°€, ì™¼ìª½=íš¨ìœ¨ ê°ì†Œ)
                        """)
                        
                        with st.spinner("Calculating SHAP values..."):
                            explainer = shap.Explainer(best_model, X_train)
                            shap_values = explainer(X_test)
                            
                            # SHAP Summary Plot
                            fig_shap, ax_shap = plt.subplots(figsize=(10, 6))
                            shap.summary_plot(shap_values, X_test, show=False)
                            st.pyplot(fig_shap)
                            
                            # SHAP Bar Plot
                            st.markdown("**Feature Importance Ranking (SHAP based)**")
                            fig_bar, ax_bar = plt.subplots(figsize=(10, 5))
                            shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)
                            st.pyplot(fig_bar)

                        # [Tab 3: ìµœì í™” ì œì•ˆ]
                        st.markdown("---")
                        st.subheader("3. Optimization Suggestions")
                        
                        best_idx = y.idxmax()
                        st.success(f"í˜„ì¬ ë°ì´í„°ì…‹ ìµœê³  íš¨ìœ¨: **{y.max():.2f}%** (Sample ID: {best_idx})")
                        
                        feature_importance = pd.DataFrame({
                            'feature': X.columns,
                            'importance': np.abs(shap_values.values).mean(axis=0)
                        }).sort_values('importance', ascending=False)
                        
                        top_features = feature_importance['feature'].head(5).tolist()
                        
                        st.markdown("#### ğŸ”¬ í•µì‹¬ ì œì–´ ë³€ìˆ˜ (Top 5)")
                        st.write("ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì‹¤í—˜ ì¡°ê±´ì„ ë¯¸ì„¸ ì¡°ì •(Fine-tuning) í•˜ì„¸ìš”.")
                        
                        best_recipe = df_clean.loc[best_idx]
                        suggestions = []
                        
                        for feat in top_features:
                            # ì›ë˜ ì»¬ëŸ¼ ì´ë¦„ ë§¤ì¹­ ì‹œë„ (ì •ê·œì‹ ì²˜ë¦¬ ì „ ì´ë¦„ ì°¾ê¸°)
                            # ì™„ì „ ì •í™•í•œ ë§¤ì¹­ì€ ì–´ë µì§€ë§Œ, feature ì´ë¦„ì´ í¬í•¨ëœ ì›ë³¸ ì»¬ëŸ¼ì„ ì°¾ìŠµë‹ˆë‹¤.
                            original_col = feat
                            for raw_col in X_raw_origin.columns:
                                # íŠ¹ìˆ˜ë¬¸ì ì œê±°ëœ ë²„ì „ê³¼ ë¹„êµ
                                cleaned_raw = re.sub(r'[^\w\s]', '_', str(raw_col))
                                cleaned_raw = re.sub(r'\s+', '_', cleaned_raw)
                                if cleaned_raw == feat:
                                    original_col = raw_col
                                    break
                            
                            current_val = best_recipe.get(original_col, "N/A")
                            
                            suggestions.append({
                                "Rank": top_features.index(feat) + 1,
                                "Feature (Cleaned)": feat,
                                "Original Feature": original_col,
                                "Best Sample Value": current_val,
                                "Action": "SHAP ê·¸ë˜í”„ë¥¼ ì°¸ì¡°í•˜ì—¬ ìµœì í™” ë°©í–¥(ì¦ê°€/ê°ì†Œ) ì„¤ì •"
                            })
                        
                        st.table(pd.DataFrame(suggestions))

                    except Exception as e:
                        st.error(f"ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                        st.error("ë°ì´í„°ì˜ ì»¬ëŸ¼ëª…ì— íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜, ë°ì´í„°ì…‹ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

else:
    st.info("ğŸ‘ˆ Please upload your data file to start.")
    st.markdown("""
    ### ğŸ“š Reference
    ë³¸ ì‹œìŠ¤í…œì€ ë‹¤ìŒê³¼ ê°™ì€ ìµœì‹  ì—°êµ¬ ë°©ë²•ë¡ ì„ ë”°ë¦…ë‹ˆë‹¤:
    1.  **XGBoost Algorithm**: Tabular dataì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” Tree-based ensemble ëª¨ë¸.
    2.  **SHAP (SHapley Additive exPlanations)**: ë¸”ë™ë°•ìŠ¤ ëª¨ë¸ì˜ ë‚´ë¶€ ì‘ë™ ì›ë¦¬ë¥¼ ê²Œì„ ì´ë¡ ìœ¼ë¡œ í•´ì„í•˜ì—¬ ê³¼í•™ì  í†µì°° ì œê³µ.
    3.  **Cross-Validation**: 5-Fold êµì°¨ ê²€ì¦ì„ í†µí•œ ì‹ ë¢°ì„± ìˆëŠ” ì„±ëŠ¥ í‰ê°€.
    """)
