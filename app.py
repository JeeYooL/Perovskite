import streamlit as st
import pandas as pd
import numpy as np

# -------------------------------------------------------------------
# [ì„¤ì •] Matplotlib ë°±ì—”ë“œ (ìŠ¤ë ˆë“œ ì¶©ëŒ ë°©ì§€)
# -------------------------------------------------------------------
import matplotlib
matplotlib.use('Agg') # ì„œë²„ ì „ìš©(GUI ì—†ìŒ) ëª¨ë“œë¡œ ì„¤ì •
import matplotlib.pyplot as plt

import seaborn as sns
import io
import re

# ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ (SEM ë¶„ì„ìš©)
import cv2

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler

# ëª¨ë¸
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern, RBF, ConstantKernel, WhiteKernel

# ì„¤ëª… ê°€ëŠ¥í•œ AI
import shap

# -------------------------------------------------------------------
# í˜ì´ì§€ ì„¤ì •
# -------------------------------------------------------------------
st.set_page_config(
    page_title="Perovskite AI Lab V7.1 (Bandgap)",
    page_icon="âš—ï¸",
    layout="wide"
)

# CSS ìŠ¤íƒ€ì¼ ì»¤ìŠ¤í…€
st.markdown("""
    <style>
    .main { background-color: #ffffff; }
    h1, h2, h3 { color: #003366; font-family: 'Arial', sans-serif; }
    
    /* íŒŒì¼ ì—…ë¡œë” ì»´íŒ©íŠ¸í•˜ê²Œ ë§Œë“¤ê¸° */
    [data-testid='stFileUploader'] {
        padding-top: 0px;
        padding-bottom: 0px;
        margin-bottom: 0px;
    }
    [data-testid='stFileUploader'] section {
        padding: 0px;
        min-height: 40px; /* ë†’ì´ ìµœì†Œí™” */
        background-color: #f8f9fa;
        border: 1px dashed #ced4da;
    }
    /* ì—…ë¡œë“œëœ íŒŒì¼ ì´ë¦„ í°íŠ¸ ì¤„ì´ê¸° */
    [data-testid='stFileUploader'] section > div {
        padding: 2px;
    }
    div[data-testid="stMarkdownContainer"] p {
        font-size: 0.9rem;
    }
    
    /* í…Œì´ë¸” í—¤ë” ìŠ¤íƒ€ì¼ */
    .upload-header {
        font-weight: bold;
        text-align: center;
        background-color: #e9ecef;
        padding: 5px;
        border-radius: 5px;
        margin-bottom: 5px;
        font-size: 0.9rem;
    }
    
    /* ìƒ˜í”Œ ID ì…€ ìŠ¤íƒ€ì¼ */
    .sample-id-cell {
        display: flex;
        align-items: center;
        justify-content: center;
        height: 42px; /* ì—…ë¡œë” ë†’ì´ì™€ ë§ì¶¤ */
        font-weight: bold;
        color: #2c3e50;
        background-color: #f1f3f5;
        border-radius: 4px;
        font-size: 0.9rem;
    }
    
    .bottom-spacer { height: 100px; }
    </style>
""", unsafe_allow_html=True)

if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None

# -------------------------------------------------------------------
# í•¨ìˆ˜ ì •ì˜
# -------------------------------------------------------------------

def load_data(uploaded_files):
    """ë©”ì¸ ë°ì´í„° íŒŒì¼ ë¡œë“œ"""
    all_dfs = []
    for uploaded_file in uploaded_files:
        try:
            if uploaded_file.name.endswith('.csv'):
                try:
                    df = pd.read_csv(uploaded_file, encoding='utf-8')
                except UnicodeDecodeError:
                    df = pd.read_csv(uploaded_file, encoding='cp949')
                all_dfs.append(df)
            elif uploaded_file.name.endswith('.xlsx'):
                df = pd.read_excel(uploaded_file)
                all_dfs.append(df)
        except Exception as e:
            st.error(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({uploaded_file.name}): {e}")
    
    if all_dfs:
        return pd.concat(all_dfs, ignore_index=True)
    return None

def extract_features_from_spectra(file, data_type):
    """
    XRD, PL, TRPL ë“± ìŠ¤í™íŠ¸ëŸ¼ ë°ì´í„°(X, Y)ì—ì„œ í•µì‹¬ Feature ì¶”ì¶œ
    + [ì‹ ê·œ] PL ë°ì´í„°ì¸ ê²½ìš° Bandgap ìë™ ê³„ì‚° ì¶”ê°€
    """
    try:
        # íŒŒì¼ ë‚´ìš© ì½ê¸° (íŒŒì‹± ë¡œì§)
        file.seek(0)
        try:
            content = file.read().decode('utf-8')
        except UnicodeDecodeError:
            file.seek(0)
            content = file.read().decode('cp949', errors='ignore')
            
        lines = content.splitlines()
        
        # ë°ì´í„° ì‹œì‘ ë¼ì¸ ì°¾ê¸°
        data_start_idx = 0
        is_data_found = False
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line: continue
            parts = re.split(r'[,\t\s]+', line)
            parts = [p for p in parts if p] 
            
            if len(parts) >= 2:
                try:
                    float(parts[0])
                    float(parts[1])
                    data_start_idx = i
                    is_data_found = True
                    break
                except ValueError:
                    continue
        
        if not is_data_found:
            return None
            
        from io import StringIO
        data_str = "\n".join(lines[data_start_idx:])
        df = pd.read_csv(StringIO(data_str), sep=None, engine='python', header=None)

        if df.shape[1] < 2:
            return None

        x = pd.to_numeric(df.iloc[:, 0], errors='coerce').dropna()
        y = pd.to_numeric(df.iloc[:, 1], errors='coerce').dropna()
        
        common_idx = x.index.intersection(y.index)
        x = x.loc[common_idx].values
        y = y.loc[common_idx].values

        features = {}
        
        # 1. Max Intensity & Peak Position
        max_idx = np.argmax(y)
        max_y = y[max_idx]
        max_x = x[max_idx] # Peak Position (nm or degree)

        features[f"{data_type}_Peak_Pos"] = max_x
        features[f"{data_type}_Max_Int"] = max_y

        # [ì‹ ê·œ ê¸°ëŠ¥] PL ë°ì´í„°ì¼ ê²½ìš° Bandgap(eV) ê³„ì‚°
        # ê³µì‹: Energy (eV) = 1240 / Wavelength (nm)
        if data_type == "PL" and max_x > 0:
            features[f"{data_type}_Bandgap_eV"] = 1240.0 / max_x

        # 2. FWHM (ë°˜ì¹˜í­)
        half_max = max_y / 2.0
        sort_idx = np.argsort(x)
        x_sorted = x[sort_idx]
        y_sorted = y[sort_idx]
        
        # í”¼í¬ê°€ ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¨ìˆœí™”ëœ ë¡œì§ ì‚¬ìš© (ìµœëŒ€ í”¼í¬ ê¸°ì¤€)
        # 1. ìµœëŒ€ê°’ë³´ë‹¤ ì™¼ìª½/ì˜¤ë¥¸ìª½ ë°ì´í„° ë¶„ë¦¬
        #    (x_sortedì—ì„œ max_xì˜ ìœ„ì¹˜ë¥¼ ì°¾ìŒ)
        #    Note: searchsortedëŠ” ì •ë ¬ëœ ë°°ì—´ì—ì„œë§Œ ì‘ë™
        try:
            # ì‹¤ì œ max_xì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì¸ë±ìŠ¤ê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°€ì¥ ê°€ê¹Œìš´ ê°’ ì°¾ê¸°
            max_pos_idx = np.argmin(np.abs(x_sorted - max_x))
            
            left_x = x_sorted[:max_pos_idx]
            left_y = y_sorted[:max_pos_idx]
            right_x = x_sorted[max_pos_idx:]
            right_y = y_sorted[max_pos_idx:]

            fwhm = 0
            if len(left_y) > 0 and len(right_y) > 0:
                idx_l = np.argmin(np.abs(left_y - half_max))
                idx_r = np.argmin(np.abs(right_y - half_max))
                fwhm = right_x[idx_r] - left_x[idx_l]
            features[f"{data_type}_FWHM"] = fwhm
        except:
            features[f"{data_type}_FWHM"] = 0

        # 3. Area
        area = np.trapz(y, x)
        features[f"{data_type}_Area"] = area

        return features

    except Exception as e:
        return None

def extract_features_from_sem(file):
    """
    SEM ì´ë¯¸ì§€ì—ì„œ Grain Size ë¶„ì„ (OpenCV í™œìš©)
    """
    try:
        file_bytes = np.asarray(bytearray(file.read()), dtype=np.uint8)
        img = cv2.imdecode(file_bytes, cv2.IMREAD_GRAYSCALE)
        
        if img is None:
            return None

        # ì „ì²˜ë¦¬
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        cl1 = clahe.apply(img)
        blurred = cv2.GaussianBlur(cl1, (5, 5), 0)

        # ì´ì§„í™” ë° ìœ¤ê³½ì„ 
        ret, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        kernel = np.ones((3,3), np.uint8)
        opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)
        contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        grain_areas = []
        for cnt in contours:
            area = cv2.contourArea(cnt)
            if area > 10: 
                grain_areas.append(area)
        
        features = {}
        if len(grain_areas) > 0:
            avg_area = np.mean(grain_areas)
            avg_diameter = np.sqrt(4 * avg_area / np.pi)
            
            features["SEM_Grain_Count"] = len(grain_areas)
            features["SEM_Avg_Size_px"] = avg_diameter
        else:
            features["SEM_Grain_Count"] = 0
            features["SEM_Avg_Size_px"] = 0
            
        return features

    except Exception as e:
        return None

def clean_column_names(df):
    df.columns = df.columns.str.strip()
    return df

def detect_target_column(df):
    candidates = [c for c in df.columns if 'PCE' in c.upper()]
    if candidates:
        return candidates[0]
    return df.columns[-1] if not df.empty else None

def preprocess_data(df, target_column):
    df_cleaned = df.dropna(subset=[target_column]).copy()
    if len(df_cleaned) == 0: return None, None, None, None

    drop_keywords = ['PCE', 'Voc', 'Jsc', 'FF', 'Rs', 'Rsh', 'Scan', 'Sample', 'File', 'Unnamed']
    cols_to_drop = [target_column]
    for col in df_cleaned.columns:
        if col == target_column: continue
        for kw in drop_keywords:
            if kw in col:
                cols_to_drop.append(col)
                break
    
    X_raw = df_cleaned.drop(columns=cols_to_drop, errors='ignore')
    y = df_cleaned[target_column]
    
    X_numeric = X_raw.select_dtypes(exclude=['object'])
    X_categorical = X_raw.select_dtypes(include=['object'])
    
    all_processed = [X_numeric]
    for col in X_categorical.columns:
        binarized = X_categorical[col].fillna('').astype(str).str.get_dummies(sep=' + ')
        binarized = binarized.add_prefix(f"{col}_")
        all_processed.append(binarized)
        
    X_processed = pd.concat(all_processed, axis=1).fillna(0)
    X_processed.columns = X_processed.columns.str.replace(r'[^\w\s]', '_', regex=True).str.replace(r'\s+', '_', regex=True)
    
    # ì¤‘ë³µ ì»¬ëŸ¼ ì²˜ë¦¬
    if X_processed.columns.duplicated().any():
        new_columns = []
        seen = {}
        for col in X_processed.columns:
            if col in seen:
                seen[col] += 1
                new_columns.append(f"{col}_{seen[col]}")
            else:
                seen[col] = 0
                new_columns.append(col)
        X_processed.columns = new_columns
    
    try:
        X_processed = X_processed.astype(float)
    except ValueError:
        for col in X_processed.columns:
            X_processed[col] = pd.to_numeric(X_processed[col], errors='coerce').fillna(0)

    return X_processed, y, df_cleaned, X_raw

# -------------------------------------------------------------------
# ë©”ì¸ UI
# -------------------------------------------------------------------

st.title("âš—ï¸ Perovskite AI Lab V7.1 (Physics-Informed)")
st.write("ê³µì •-ë¬¼ì„± í†µí•© ë¶„ì„ (PL ë°´ë“œê°­ ìë™ ê³„ì‚° í¬í•¨)")
st.markdown("---")

# ì‚¬ì´ë“œë°”: ë©”ì¸ ë°ì´í„° ì—…ë¡œë“œ (í•­ìƒ í‘œì‹œ)
with st.sidebar:
    st.header("ğŸ“‚ 1. Main Recipe Data")
    uploaded_files = st.file_uploader("ë©”ì¸ CSV/Excel (Sample ID í•„ìˆ˜)", type=['csv', 'xlsx'], accept_multiple_files=True, key="main")
    
    st.markdown("---")
    if st.button("ğŸ”„ ê²°ê³¼ ì´ˆê¸°í™”"):
        st.session_state.analysis_results = None
        st.rerun()

# ë©”ì¸ ë¡œì§
if uploaded_files:
    raw_df = load_data(uploaded_files)
    
    if raw_df is not None:
        raw_df = clean_column_names(raw_df)
        
        # --------------------------------------------------------------------
        # í™”ë©´ ë¶„í•  ë ˆì´ì•„ì›ƒ (50:50)
        # --------------------------------------------------------------------
        col_left, col_right = st.columns([1, 1], gap="medium")

        # ====================================================================
        # [ì™¼ìª½] ì¶”ê°€ ë°ì´í„° ì—…ë¡œë“œ (í…Œì´ë¸” í˜•ì‹)
        # ====================================================================
        with col_left:
            st.subheader("ğŸ”¬ 2. Characterization Data Upload")
            st.info("ìƒ˜í”Œë³„ XRD, PL, TRPL, SEM ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”. (PL ì—…ë¡œë“œ ì‹œ ë°´ë“œê°­ ìë™ ê³„ì‚°)")
            
            if 'Sample' in raw_df.columns:
                try:
                    sample_ids = sorted(raw_df['Sample'].unique(), key=lambda x: float(x) if str(x).replace('.','',1).isdigit() else str(x))
                except:
                    sample_ids = sorted(raw_df['Sample'].astype(str).unique())
                
                # ê²€ìƒ‰ì°½
                search_term = st.text_input("ğŸ” Sample ID ê²€ìƒ‰", placeholder="ìƒ˜í”Œ ë²ˆí˜¸ ì…ë ¥...")
                filtered_samples = sample_ids
                if search_term:
                    filtered_samples = [s for s in sample_ids if search_term.lower() in str(s).lower()]

                st.markdown("<br>", unsafe_allow_html=True)

                # 1. í…Œì´ë¸” í—¤ë” (5ì—´)
                h_c1, h_c2, h_c3, h_c4, h_c5 = st.columns([1, 2, 2, 2, 2])
                h_c1.markdown("<div class='upload-header'>ID</div>", unsafe_allow_html=True)
                h_c2.markdown("<div class='upload-header'>XRD</div>", unsafe_allow_html=True)
                h_c3.markdown("<div class='upload-header'>PL</div>", unsafe_allow_html=True)
                h_c4.markdown("<div class='upload-header'>TRPL</div>", unsafe_allow_html=True)
                h_c5.markdown("<div class='upload-header'>SEM</div>", unsafe_allow_html=True)
                
                # 2. í…Œì´ë¸” í–‰ ë°˜ë³µ ìƒì„±
                additional_features_list = []
                
                for s_id in filtered_samples:
                    row_c1, row_c2, row_c3, row_c4, row_c5 = st.columns([1, 2, 2, 2, 2])
                    
                    with row_c1:
                        st.markdown(f"<div class='sample-id-cell'>{s_id}</div>", unsafe_allow_html=True)
                    
                    f_xrd = row_c2.file_uploader("XRD", key=f"xrd_{s_id}", type=['csv', 'txt', 'dat'], label_visibility="collapsed")
                    f_pl = row_c3.file_uploader("PL", key=f"pl_{s_id}", type=['csv', 'txt', 'dat'], label_visibility="collapsed")
                    f_trpl = row_c4.file_uploader("TRPL", key=f"trpl_{s_id}", type=['csv', 'txt', 'dat'], label_visibility="collapsed")
                    f_sem = row_c5.file_uploader("SEM", key=f"sem_{s_id}", type=['jpg', 'jpeg', 'png', 'tif', 'tiff'], label_visibility="collapsed")
                    
                    current_feats = {'Sample': s_id}
                    
                    if f_xrd:
                        feats = extract_features_from_spectra(f_xrd, "XRD")
                        if feats: current_feats.update(feats)
                    if f_pl:
                        feats = extract_features_from_spectra(f_pl, "PL")
                        if feats: current_feats.update(feats)
                    if f_trpl:
                        feats = extract_features_from_spectra(f_trpl, "TRPL")
                        if feats: current_feats.update(feats)
                    if f_sem:
                        feats = extract_features_from_sem(f_sem)
                        if feats: current_feats.update(feats)
                    
                    if len(current_feats) > 1:
                        additional_features_list.append(current_feats)
                
                # ë³‘í•© ë¡œì§
                if additional_features_list:
                    add_df = pd.DataFrame(additional_features_list)
                    try:
                        raw_df['Sample'] = raw_df['Sample'].astype(int)
                        add_df['Sample'] = add_df['Sample'].astype(int)
                    except:
                        raw_df['Sample'] = raw_df['Sample'].astype(str)
                        add_df['Sample'] = add_df['Sample'].astype(str)
                    
                    raw_df = pd.merge(raw_df, add_df, on='Sample', how='left')
                    st.success(f"âœ… ì´ {len(additional_features_list)}ê°œ ìƒ˜í”Œì˜ ì™¸ë¶€ ë°ì´í„° ë³‘í•© ì™„ë£Œ")

            else:
                st.error("ë©”ì¸ ë°ì´í„°ì— 'Sample' ì»¬ëŸ¼ì´ ì—†ì–´ í…Œì´ë¸”ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ====================================================================
        # [ì˜¤ë¥¸ìª½] ë¶„ì„ ì„¤ì • ë° ê²°ê³¼
        # ====================================================================
        with col_right:
            st.subheader("âš™ï¸ Analysis & Results")
            st.write(f"âœ… ì´ **{len(raw_df)}**ê°œ ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ë¨")
            
            # ì™¸ë¶€ ë³€ìˆ˜ í™•ì¸
            ext_cols = [c for c in raw_df.columns if c.startswith(('XRD_', 'PL_', 'TRPL_', 'SEM_'))]
            if ext_cols:
                st.caption(f"âœ¨ ì¶”ì¶œëœ ë³€ìˆ˜: {', '.join(ext_cols)}")
            
            # ë°´ë“œê°­ ê³„ì‚° í™•ì¸ ë©”ì‹œì§€
            if 'PL_Bandgap_eV' in raw_df.columns:
                st.info("ğŸ’¡ **Physics-Informed:** PL ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **Bandgap (eV)**ì´ ìë™ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤!")

            with st.expander("í†µí•© ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                st.dataframe(raw_df.head())
            
            st.markdown("---")

            # ë¶„ì„ ì„¤ì • UI
            lc1, lc2, lc3 = st.columns(3)
            with lc1:
                target_col = st.selectbox("íƒ€ê²Ÿ ë³€ìˆ˜", options=raw_df.columns, index=list(raw_df.columns).index(detect_target_column(raw_df)) if detect_target_column(raw_df) else 0)
            with lc2:
                model_choice = st.selectbox("ML ëª¨ë¸", ["XGBoost (Recommended)", "Random Forest", "Gaussian Process"])
            with lc3:
                test_ratio = st.slider("í…ŒìŠ¤íŠ¸ ë¹„ìœ¨", 0.1, 0.5, 0.2)

            # ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼
            st.markdown("<br>", unsafe_allow_html=True)
            if st.button("ğŸš€ AI ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True):
                with st.spinner(f"{model_choice} ìµœì í™” ëª¨ë¸ êµ¬ë™ ì¤‘..."):
                    try:
                        X, y, df_clean, X_raw_origin = preprocess_data(raw_df, target_col)
                        
                        if X is None:
                            st.error("ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨")
                        else:
                            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=42)
                            
                            model = None
                            is_tree_model = False
                            
                            if "XGBoost" in model_choice:
                                is_tree_model = True
                                xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', n_jobs=-1, random_state=42)
                                search = GridSearchCV(xgb_reg, {'n_estimators':[100,200], 'max_depth':[3,5], 'learning_rate':[0.05,0.1]}, cv=3, scoring='neg_mean_absolute_error', error_score='raise')
                                search.fit(X_train, y_train)
                                model = search.best_estimator_
                            elif "Random Forest" in model_choice:
                                is_tree_model = True
                                rf_reg = RandomForestRegressor(random_state=42, n_jobs=-1)
                                search = GridSearchCV(rf_reg, {'n_estimators':[100,200], 'max_depth':[10,None]}, cv=3, scoring='neg_mean_absolute_error')
                                search.fit(X_train, y_train)
                                model = search.best_estimator_
                            elif "Gaussian Process" in model_choice:
                                scaler_X = StandardScaler()
                                X_train_scaled = scaler_X.fit_transform(X_train)
                                X_test_scaled = scaler_X.transform(X_test)
                                kernel = 1.0 * RBF(1.0) + WhiteKernel(1.0)
                                gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5, random_state=42)
                                gp.fit(X_train_scaled, y_train)
                                model = gp
                                model.custom_predict_std = lambda X_in: gp.predict(scaler_X.transform(X_in), return_std=True)

                            if "Gaussian Process" in model_choice:
                                y_pred, y_std = model.custom_predict_std(X_test)
                            else:
                                y_pred = model.predict(X_test)
                                y_std = None
                                
                            st.session_state.analysis_results = {
                                "model": model, "r2": r2_score(y_test, y_pred), "mae": mean_absolute_error(y_test, y_pred),
                                "y_test": y_test, "y_pred": y_pred, "y_std": y_std, "X_test": X_test, "X_train": X_train,
                                "X": X, "y": y, "target_col": target_col, "df_clean": df_clean,
                                "X_raw_origin": X_raw_origin, "model_choice": model_choice, "is_tree_model": is_tree_model
                            }
                    except Exception as e:
                        st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

            # ê²°ê³¼ ë¦¬í¬íŠ¸
            if st.session_state.analysis_results:
                res = st.session_state.analysis_results
                st.markdown("---")
                
                t1, t2, t3 = st.tabs(["ğŸ“Š ì„±ëŠ¥ í‰ê°€", "ğŸ” ì¤‘ìš”ë„ ë¶„ì„", "ğŸ’¡ ìµœì í™” ì œì•ˆ"])
                
                with t1:
                    col1, col2 = st.columns(2)
                    col1.metric("ê²°ì •ê³„ìˆ˜ (RÂ²)", f"{res['r2']:.4f}")
                    col2.metric("ì˜¤ì°¨ (MAE)", f"{res['mae']:.4f}")
                    fig, ax = plt.subplots(figsize=(6,5))
                    ax.scatter(res['y_test'], res['y_pred'], alpha=0.7, edgecolors='k')
                    ax.plot([res['y'].min(), res['y'].max()], [res['y'].min(), res['y'].max()], 'r--', lw=2)
                    ax.set_xlabel("Actual"); ax.set_ylabel("Predicted")
                    st.pyplot(fig)

                with t2:
                    importances = None
                    if res['is_tree_model']:
                        try:
                            explainer = shap.Explainer(res['model'], res['X_train'])
                            shap_values = explainer(res['X_test'])
                            fig, ax = plt.subplots()
                            shap.summary_plot(shap_values, res['X_test'], show=False)
                            st.pyplot(fig)
                            importances = np.abs(shap_values.values).mean(axis=0)
                        except:
                            importances = res['model'].feature_importances_
                    else:
                        full_corr = res['X'].copy()
                        full_corr['Target'] = res['y'].values
                        importances = np.abs(full_corr.corr()['Target'].drop('Target').values)
                    
                with t3:
                    best_idx = res['y'].idxmax()
                    st.info(f"ğŸ† Best Sample: **ID {best_idx}** ({res['y'].max():.2f})")
                    
                    feat_imp_df = pd.DataFrame({'Feature': res['X'].columns, 'Imp': list(importances)})
                    top_feats = feat_imp_df.sort_values('Imp', ascending=False).head(5)['Feature'].tolist()
                    
                    best_recipe = res['df_clean'].loc[best_idx]
                    suggestions = []
                    for feat in top_feats:
                        orig = feat
                        for raw_c in res['X_raw_origin'].columns:
                            if re.sub(r'[^\w]', '_', str(raw_c)) in feat:
                                orig = raw_c
                                break
                        val = best_recipe.get(orig, "N/A")
                        
                        # Context
                        parts = str(orig).split('_')
                        prefix = "_".join(parts[:2]) if len(parts)>=2 else parts[0]
                        ctx = [f"{c.replace(prefix,'').strip('_')}:{best_recipe[c]}" for c in best_recipe.index if c!=orig and str(c).startswith(prefix) and pd.notna(best_recipe[c])]
                        
                        suggestions.append({"ìˆœìœ„": top_feats.index(feat)+1, "ì¤‘ìš” ë³€ìˆ˜": feat, "ìµœê³  íš¨ìœ¨ ì¡°ê±´": val, "ì„¸ë¶€ ì¡°ê±´": " | ".join(ctx) if ctx else "-"})
                    st.table(pd.DataFrame(suggestions))
                
                st.markdown('<div class="bottom-spacer"></div>', unsafe_allow_html=True)
else:
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë©”ì¸ ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
