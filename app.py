import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import io
import re

# ë¨¸ì‹ ëŸ¬ë‹ & ì„¤ëª… ê°€ëŠ¥í•œ AI(XAI) ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import r2_score, mean_absolute_error
import xgboost as xgb
import shap

# -------------------------------------------------------------------
# í˜ì´ì§€ ì„¤ì •
# -------------------------------------------------------------------
st.set_page_config(
    page_title="Perovskite AI Lab (XGBoost + SHAP)",
    page_icon="ğŸ§ª",
    layout="wide"
)

# ìŠ¤íƒ€ì¼ ì»¤ìŠ¤í…€
st.markdown("""
    <style>
    .main { background-color: #ffffff; }
    h1, h2, h3 { color: #003366; font-family: 'Arial', sans-serif; }
    .stMetric { background-color: #f0f2f6; padding: 10px; border-radius: 5px; }
    </style>
""", unsafe_allow_html=True)

# -------------------------------------------------------------------
# í•¨ìˆ˜ ì •ì˜
# -------------------------------------------------------------------

def load_data(uploaded_files):
    """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³‘í•©"""
    all_dfs = []
    for uploaded_file in uploaded_files:
        try:
            if uploaded_file.name.endswith('.csv'):
                # encoding ê´€ë ¨ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•œ ì‹œë„
                try:
                    df = pd.read_csv(uploaded_file, encoding='utf-8')
                except UnicodeDecodeError:
                    df = pd.read_csv(uploaded_file, encoding='cp949')
                all_dfs.append(df)
            elif uploaded_file.name.endswith('.xlsx'):
                df = pd.read_excel(uploaded_file)
                all_dfs.append(df)
        except Exception as e:
            st.error(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({uploaded_file.name}): {e}")
    
    if all_dfs:
        return pd.concat(all_dfs, ignore_index=True)
    return None

def preprocess_data(df):
    """ë°ì´í„° ì „ì²˜ë¦¬: ìœ ì—°í•œ íƒ€ê²Ÿ ì»¬ëŸ¼ ì°¾ê¸°, ê²°ì¸¡ì¹˜ ì œê±°, MLB, ì»¬ëŸ¼ëª… ì •ì œ"""
    
    # 1. ì»¬ëŸ¼ëª… ê³µë°± ì œê±° (ì‹¤ìˆ˜ ë°©ì§€)
    df.columns = df.columns.str.strip()
    
    # 2. íƒ€ê²Ÿ ì»¬ëŸ¼('PCE') ìë™ íƒìƒ‰
    # ì‚¬ìš©ìê°€ ë§í•œ 'íŠ¹ìˆ˜ë¬¸ì ì´ìŠˆ'ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì •í™•í•œ ì´ë¦„ ëŒ€ì‹  'PCE'ê°€ í¬í•¨ëœ ì»¬ëŸ¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    target_column = 'PCE (%)'
    
    if target_column not in df.columns:
        # 'PCE'ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ëœ ì»¬ëŸ¼ì„ í›„ë³´ë¡œ ì°¾ìŒ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
        pce_candidates = [c for c in df.columns if 'PCE' in c.upper()]
        
        if len(pce_candidates) > 0:
            target_column = pce_candidates[0] # ì²« ë²ˆì§¸ í›„ë³´ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì„¤ì •
            st.info(f"â„¹ï¸ '{target_column}' ì»¬ëŸ¼ì„ íƒ€ê²Ÿ ê°’(íš¨ìœ¨)ìœ¼ë¡œ ìë™ ì¸ì‹í–ˆìŠµë‹ˆë‹¤.")
        else:
            st.error("ë°ì´í„°ì—ì„œ 'PCE' ê´€ë ¨ ì»¬ëŸ¼(ì˜ˆ: PCE (%), PCE)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CSV íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return None, None, None, None

    # íƒ€ê²Ÿ ê°’ì´ ì—†ëŠ” í–‰ ì œê±°
    df_cleaned = df.dropna(subset=[target_column]).copy()
    
    # Data Leakage ë°©ì§€: ê²°ê³¼ê°’ ì»¬ëŸ¼ ì œì™¸
    # PCE ì™¸ì— ì „ì••, ì „ë¥˜ ë“± ê²°ê³¼ ì§€í‘œë“¤ë„ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤.
    drop_keywords = ['PCE', 'Voc', 'Jsc', 'FF', 'Rs', 'Rsh', 'Scan Direction', 'Sample', 'File']
    
    # ì‚­ì œí•  ì»¬ëŸ¼ ì°¾ê¸°
    cols_to_drop = []
    for col in df_cleaned.columns:
        # íƒ€ê²Ÿ ì»¬ëŸ¼ì€ ë‹¹ì—°íˆ yë¡œ ê°€ë¯€ë¡œ Xì—ì„œëŠ” ì œì™¸
        if col == target_column:
            cols_to_drop.append(col)
            continue
            
        # ê²°ê³¼ ì§€í‘œë“¤ í¬í•¨ ì—¬ë¶€ í™•ì¸
        for kw in drop_keywords:
            if kw in col:
                cols_to_drop.append(col)
                break
                
    # Unnamed ì»¬ëŸ¼ ì œê±°
    cols_to_drop += [c for c in df_cleaned.columns if 'Unnamed' in c]
    
    X_raw = df_cleaned.drop(columns=cols_to_drop, errors='ignore')
    y = df_cleaned[target_column]
    
    # ìˆ˜ì¹˜í˜•/ë²”ì£¼í˜• ë¶„ë¦¬
    X_numeric = X_raw.select_dtypes(exclude=['object'])
    X_categorical = X_raw.select_dtypes(include=['object'])
    
    all_processed_dfs = [X_numeric]

    for col in X_categorical.columns:
        # 'FAI + MACl' ê°™ì€ ë³µí•© ì¡°ì„±ì„ ê°œë³„ ì„±ë¶„ìœ¼ë¡œ ë¶„ë¦¬
        binarized = X_categorical[col].fillna('').astype(str).str.get_dummies(sep=' + ')
        binarized = binarized.add_prefix(f"{col}_")
        all_processed_dfs.append(binarized)
        
    X_processed = pd.concat(all_processed_dfs, axis=1).fillna(0)
    
    # [ì¤‘ìš”] XGBoost í˜¸í™˜ì„±ì„ ìœ„í•´ **ëª¨ë“  ì»¬ëŸ¼ëª…**ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
    # ì´ ì‘ì—…ì€ ë°ì´í„°ë¥¼ ë‹¤ ë§Œë“  í›„(X_processed)ì— ìˆ˜í–‰í•˜ë¯€ë¡œ,
    # ì´ˆê¸°ì— 'PCE (%)' ì»¬ëŸ¼ì„ ì°¾ëŠ” ê³¼ì •ì—ëŠ” ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.
    X_processed.columns = X_processed.columns.str.replace(r'[^\w\s]', '_', regex=True).str.replace(r'\s+', '_', regex=True)
    
    return X_processed, y, df_cleaned, X_raw

# -------------------------------------------------------------------
# ë©”ì¸ UI
# -------------------------------------------------------------------

st.title("ğŸ§ª Perovskite AI Lab: XGBoost & SHAP Analysis")
st.markdown("""
ìµœì‹  ì—°êµ¬ íŠ¸ë Œë“œ(Science, Nature Energy ë“±)ë¥¼ ë°˜ì˜í•˜ì—¬ **XGBoost(ê³ ì„±ëŠ¥ ë¶€ìŠ¤íŒ…)** ëª¨ë¸ê³¼ **SHAP(ì„¤ëª… ê°€ëŠ¥í•œ AI)** ê¸°ë²•ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.
""")
st.markdown("---")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("1. Data Upload")
    uploaded_files = st.file_uploader("Upload CSV/Excel", type=['csv', 'xlsx'], accept_multiple_files=True)
    
    st.header("2. Model Settings")
    test_size = st.slider("Test Set Ratio", 0.1, 0.4, 0.2, 0.05)
    cv_folds = st.slider("CV Folds", 2, 10, 5)
    
    st.markdown("---")
    st.info("ğŸ’¡ **XGBoost**ëŠ” í˜ë¡œë¸ŒìŠ¤ì¹´ì´íŠ¸ ê³µì • ë°ì´í„°ì™€ ê°™ì€ ì •í˜• ë°ì´í„°(Tabular Data)ì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")

if uploaded_files:
    raw_df = load_data(uploaded_files)
    
    if raw_df is not None:
        st.write(f"âœ… Loaded **{len(raw_df)}** samples.")
        
        # ê°„ë‹¨í•œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ë””ë²„ê¹…ìš©)
        with st.expander("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 5í–‰)"):
            st.dataframe(raw_df.head())
        
        if st.button("ğŸš€ Run AI Analysis (Train & Explain)"):
            with st.spinner('Preprocessing data & optimizing XGBoost model...'):
                
                # 1. ì „ì²˜ë¦¬ (ìœ ì—°í•œ ì»¬ëŸ¼ íƒìƒ‰ ì ìš©ë¨)
                X, y, df_clean, X_raw_origin = preprocess_data(raw_df)
                
                if X is not None:
                    # 2. ë°ì´í„° ë¶„í• 
                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
                    
                    # 3. XGBoost ëª¨ë¸ ì„¤ì •
                    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_jobs=-1, random_state=42)
                    
                    param_grid = {
                        'n_estimators': [100, 200, 300],
                        'learning_rate': [0.01, 0.05, 0.1],
                        'max_depth': [3, 5],
                        'subsample': [0.8, 1.0]
                    }
                    
                    grid_search = GridSearchCV(
                        xgb_model, 
                        param_grid, 
                        cv=cv_folds, 
                        scoring='neg_mean_absolute_error',
                        verbose=1,
                        error_score='raise'
                    )
                    
                    try:
                        grid_search.fit(X_train, y_train)
                        best_model = grid_search.best_estimator_
                        
                        # [Tab 1: ì„±ëŠ¥]
                        st.subheader("1. Model Performance")
                        col1, col2, col3 = st.columns(3)
                        
                        y_pred = best_model.predict(X_test)
                        r2 = r2_score(y_test, y_pred)
                        mae = mean_absolute_error(y_test, y_pred)
                        cv_r2 = cross_val_score(best_model, X, y, cv=cv_folds, scoring='r2').mean()
                        
                        col1.metric("Test RÂ² Score", f"{r2:.4f}")
                        col2.metric("Mean Absolute Error", f"{mae:.4f} %")
                        col3.metric("Cross-Validation RÂ²", f"{cv_r2:.4f}")
                        
                        # ì˜ˆì¸¡ ê·¸ë˜í”„
                        fig, ax = plt.subplots(figsize=(8, 4))
                        ax.scatter(y_test, y_pred, alpha=0.6, color='#2c3e50', edgecolors='w')
                        ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
                        ax.set_xlabel("Experimental PCE (%)")
                        ax.set_ylabel("Predicted PCE (%)")
                        ax.set_title("Prediction Accuracy")
                        ax.grid(True, alpha=0.3)
                        st.pyplot(fig)
                        
                        # [Tab 2: SHAP ë¶„ì„ (XAI)]
                        st.markdown("---")
                        st.subheader("2. Explainable AI (SHAP Analysis)")
                        st.markdown("""
                        **SHAP Summary Plot**ì€ ê° ê³µì • ë³€ìˆ˜ê°€ íš¨ìœ¨ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
                        * **ì ì˜ ìƒ‰ìƒ**: ë³€ìˆ˜ì˜ ê°’ (ë¹¨ê°•=ë†’ìŒ, íŒŒë‘=ë‚®ìŒ)
                        * **Xì¶• ìœ„ì¹˜**: íš¨ìœ¨ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ (ì˜¤ë¥¸ìª½=íš¨ìœ¨ ì¦ê°€, ì™¼ìª½=íš¨ìœ¨ ê°ì†Œ)
                        """)
                        
                        with st.spinner("Calculating SHAP values..."):
                            explainer = shap.Explainer(best_model, X_train)
                            shap_values = explainer(X_test)
                            
                            fig_shap, ax_shap = plt.subplots(figsize=(10, 6))
                            shap.summary_plot(shap_values, X_test, show=False)
                            st.pyplot(fig_shap)
                            
                            st.markdown("**Feature Importance Ranking (SHAP based)**")
                            fig_bar, ax_bar = plt.subplots(figsize=(10, 5))
                            shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)
                            st.pyplot(fig_bar)

                        # [Tab 3: ìµœì í™” ì œì•ˆ]
                        st.markdown("---")
                        st.subheader("3. Optimization Suggestions")
                        
                        best_idx = y.idxmax()
                        st.success(f"í˜„ì¬ ë°ì´í„°ì…‹ ìµœê³  íš¨ìœ¨: **{y.max():.2f}%** (Sample ID: {best_idx})")
                        
                        feature_importance = pd.DataFrame({
                            'feature': X.columns,
                            'importance': np.abs(shap_values.values).mean(axis=0)
                        }).sort_values('importance', ascending=False)
                        
                        top_features = feature_importance['feature'].head(5).tolist()
                        
                        st.markdown("#### ğŸ”¬ í•µì‹¬ ì œì–´ ë³€ìˆ˜ (Top 5)")
                        best_recipe = df_clean.loc[best_idx]
                        suggestions = []
                        
                        for feat in top_features:
                            # ì›ë³¸ ì»¬ëŸ¼ ì°¾ê¸° (ì •ì œëœ ì´ë¦„ -> ì›ë³¸ ì´ë¦„ ë§¤í•‘)
                            original_col = feat
                            # 1. ì™„ë²½ ë§¤ì¹­ ì‹œë„
                            if feat in X_raw_origin.columns:
                                original_col = feat
                            else:
                                # 2. ë¶€ë¶„ ë§¤ì¹­ (íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ ì „í›„ ë¹„êµ)
                                for raw_col in X_raw_origin.columns:
                                    cleaned_raw = re.sub(r'[^\w\s]', '_', str(raw_col))
                                    cleaned_raw = re.sub(r'\s+', '_', cleaned_raw)
                                    if cleaned_raw == feat:
                                        original_col = raw_col
                                        break
                            
                            current_val = best_recipe.get(original_col, "N/A")
                            
                            suggestions.append({
                                "Rank": top_features.index(feat) + 1,
                                "Feature (AI Name)": feat,
                                "Original Column": original_col,
                                "Best Value": current_val,
                                "Action": "SHAP ê·¸ë˜í”„ ì°¸ì¡°í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •"
                            })
                        
                        st.table(pd.DataFrame(suggestions))

                    except Exception as e:
                        st.error(f"ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                        st.write("íŒíŠ¸: ë°ì´í„°ì˜ ì–‘ì´ ë„ˆë¬´ ì ê±°ë‚˜(ìµœì†Œ 20ê°œ ì´ìƒ ê¶Œì¥), XGBoostê°€ ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” ë°ì´í„° í˜•ì‹ì´ í¬í•¨ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

else:
    st.info("ğŸ‘ˆ Please upload your data file to start.")
